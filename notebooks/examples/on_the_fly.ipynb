{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# On-The-Fly (OTF) Data Reduction\n",
    "\n",
    "This notebook shows how to use `dysh` to calibrate and grid an *On-The-Fly* (OTF) observation. See [Mangum et al. (2007)](https://ui.adsabs.harvard.edu/abs/2007A%26A...474..679M) for the background on this method.\n",
    "\n",
    "OTF observations can be calibrated in `dysh`, however, generating FITS cubes requires the use of additional applications.\n",
    "\n",
    "The workflow to go from raw data to a FITS cube would then require the following steps:\n",
    "\n",
    "1. Calibrate the data using `dysh`. This can include baseline subtraction, altough in some cases baseline removal is more effective in a FITS cube.\n",
    "\n",
    "2. Write the calibrated spectra to a format compatible with the gridding tool being used. For GBT observations the recommended format is SDFITS and the tool to grid the data is the [gbtgridder](https://github.com/GreenBankObservatory/gbtgridder/tree/release_3.0).\n",
    "\n",
    "3. Baseline subtraction in the FITS cube. This is optional, depending on the data quality, and `dysh` does not provide convenience functions for this.\n",
    "\n",
    "You can find a copy of this tutorial as a Jupyter notebook [here](https://github.com/GreenBankObservatory/dysh/blob/main/notebooks/examples/on_the_fly.ipynb) or download it by right clicking  <a href=\"https://raw.githubusercontent.com/GreenBankObservatory/dysh/refs/heads/main/notebooks/examples/on_the_fly.ipynb\" download>here</a> and selecting \"Save Link As\".\n",
    "\n",
    "## Loading Modules\n",
    "We start by loading the modules we will use for the data reduction. \n",
    "\n",
    "For display purposes, we use the static (non-interactive) `matplotlib` backend in this tutorial. However, you can tell `matplotlib` to use the `ipympl` backend to enable interactive plots. This is only needed if working on `jupyter` lab or notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "import"
    ]
   },
   "outputs": [],
   "source": [
    "# We use dysh_data to retrieve the example data set.\n",
    "from dysh.util.files import dysh_data\n",
    "\n",
    "# This is required to load and calibrate the example data set.\n",
    "from dysh.fits.gbtfitsload import GBTFITSLoad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Data Retrieval\n",
    "\n",
    "Download the example SDFITS data, if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "wget"
    ]
   },
   "outputs": [],
   "source": [
    "# example=\"otf1\" also works\n",
    "filename = dysh_data(example='mapping-L/data/TGBT17A_506_11.raw.vegas/TGBT17A_506_11.raw.vegas.A.fits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "load"
    ]
   },
   "outputs": [],
   "source": [
    "sdfits = GBTFITSLoad(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdfits.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "In this particular observation the OTF slew over the galaxy NGC6946 in scans 14-26, followed by an Off position scan. Each **SCAN**, in this case, corresponds to a row, with 61 integrations as the telescope slews slowly over the sky."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Data Reduction\n",
    "\n",
    "We use `getsigref` to calibrate the OTF scans using scan 27 as the reference position.\n",
    "`getsigref` takes as input a list of scan numbers, or a single one, and a number, or `Spectrum`, for the reference position.\n",
    "It calibrates all the input scans using\n",
    "$$\n",
    "T_{\\mathrm{A}}=T_{\\mathrm{sys}}\\frac{P_{\\mathrm{sig}}-P_{\\mathrm{ref}}}{P_{\\mathrm{ref}}},\n",
    "$$\n",
    "where $T_{\\mathrm{sys}}$ is the system temperature derived from the reference scan, $P_{\\mathrm{sig}}$ is the raw power in the input scans (the signal), and $P_{\\mathrm{ref}}$ is the raw power in the reference scan.\n",
    "\n",
    "We start by defining the variables we will use to calibrate the 21 cm-HI line observations present in this data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ifnum = 0                     # The 21 cm line is in the spectral window labeled 0.\n",
    "fdnum = 0                     # Only one feed in this data set\n",
    "ref   = 27                    # The reference (\"OFF\") scan\n",
    "scans = list(range(14,27))    # The signal (\"ON\") scans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "Now, we calibrate a single polarization.\n",
    "The processing for the second polarization should be almost identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "sb0 = sdfits.getsigref(scan=scans, ref=ref, fdnum=fdnum, ifnum=ifnum, plnum=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "The return from `getsigref` is a `ScanBlock` with `PSScan`s in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "sb0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Data Inspection\n",
    "\n",
    "After calibrating the data, we can inspect the calibrated data using the plotting methods available in `dysh`.\n",
    "First, we will extract a single spectrum from the middle of the observations and plot it. \n",
    "To get the spectrum from the middle of the OTF map, we use `len(sb0)//2` to specify the middle scan and `sb0[len(sb0)//2].nint//2` to specify the middle integration (in this case there are 61 integrations).\n",
    "We store the number of integrations in the `nint` variable to reuse it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "nint = sb0[len(sb0)//2].nint\n",
    "spec = sb0[len(sb0)//2].getspec(nint//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spec.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "A clear and strong signal is present, as well as a ~2 K continuum.\n",
    "\n",
    "Now we will plot the time average for the middle scan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_avg = sb0[len(sb0)//2].timeaverage()\n",
    "sp_avg = spec_avg.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "Again, a clear signal, but the line brightness and continuum are lower due to the dilution from the time averaging. This is because the source is smaller than the area mapped, so the signal gets averaged with empty sky."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "print_tsys",
     "test"
    ]
   },
   "source": [
    "### Baseline Subtraction\n",
    "\n",
    "If we are only interested in the spectral line, then we use baseline subtraction to remove the continuum.\n",
    "We will use an order 1 polynomial to remove the continuum, and make sure to exclude the edge channels as well as the channels with 21 cm signal during the baseline fit.\n",
    "\n",
    "We explore two approaches to continuum subtraction (baseline removal), using a model derived from a time average and deriving a baseline for each integration.\n",
    "\n",
    "#### Using a Time Average\n",
    "\n",
    "We start by using the time average of the spectra in one scan to derive a baseline model, and then we will use this baseline model to subtract the continuum from all of the integrations in the scan.\n",
    "This approach has the benefit of being faster than deriving a baseline from each integration, and the spectrum used to derive the baseline model has a higher signal-to-noise.\n",
    "However, this approach requires that the baseline be stable in time/sky position.\n",
    "If that is not the case, then there will be left over baseline/continuum in the baseline subtracted data.\n",
    "\n",
    "We start by plotting the time average as a function of channel number to determine where the 21 cm signal is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_avg.plot(xaxis_unit=\"channel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "From the plot we see that channels ~1600 to 2250 have 21 cm signal, and the edges are between 0 and 250, and 4095-250 and 4095.\n",
    "We define an exclude region with these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = [(0,250),\n",
    "           (1600,2250),\n",
    "           (4095-250,4095)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "Now do the baseline fitting, using an order 1 polynomial and removing the best fit model from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_avg.baseline(1, model=\"poly\", exclude=exclude, remove=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "Plot the baseline subtracted data.\n",
    "(Alternatively, one could use `spec_avg.plot()`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_avg.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "The continuum has been removed (the spectrum is centered around 0 K). \n",
    "We can look at the statistics to verify this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_avg.stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "The mean is around -8.6 mK, the median -4.5 mK and the rms 0.25 K."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "We can subtract this baseline model from all of the integrations using the `subtract_baseline` method of a `ScanBlock` or `Scan`.\n",
    "The input to `subtract_baseline` should be a baseline model, which can be accessed through the `baseline_model` attribute of a `Spectrum` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "sb0[len(sb0)//2].subtract_baseline(spec_avg.baseline_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "Generate a time average again to see how the data changed after the baseline subtraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_avg_bsub = sb0[len(sb0)//2].timeaverage()\n",
    "sp_avg_bsub = spec_avg_bsub.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "The time average for the middle scan of the OTF map is now centered around zero as expected.\n",
    "However, since we used the diluted time average for the middle scan as our baseline model, the spectra that cover the source still have continuum left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_bsub = sb0[len(sb0)//2].getspec(nint//2)\n",
    "sp_spec_bsub = spec_bsub.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "#### Using the Integrations\n",
    "\n",
    "Now we will repeat the calibration, but deriving a baseline model from each integration.\n",
    "This is slower, so we only do this for the middle scan of the OTF map.\n",
    "This approach has the benefit of being more flexible than the previous one, however the signal-to-noise is worse.\n",
    "\n",
    "To access the data for the integrations we will use the `calibrated` method and the `_calibrated` property of a `Scan`.\n",
    "`calibrated` returns the data for a specific integration (starting at 0) as a `Spectrum` object, while `_calibrated` is the array that contains the calibrated data for the `Scan`. \n",
    "We will use the `Spectrum` to derive the baseline, and then update the data by updating the `_calibrated` property of the `Scan`.\n",
    "\n",
    "We put the middle scan of the OTF map in a new variable `scan`, then loop over its integrations fitting a baseline model and subtracting it from the calibrated data. \n",
    "We ignore integrations that were blanked (all values would be NaN).\n",
    "We use the `math` library to check for NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math # Load the `math` library. Use this instead of `numpy` to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "scan = sb0[len(sb0)//2]\n",
    "\n",
    "for i,_c in enumerate(scan.calibrated):\n",
    "    if math.isnan(_c.data.sum()):\n",
    "        # If the sum is NaN, then skip (continue) this item.\n",
    "        # This is not a great solution, as even a single NaN value\n",
    "        # in the spectrum would cause the sum to be NaN,\n",
    "        # but there should be no NaN values for single channels\n",
    "        # in this data set.\n",
    "        continue\n",
    "    s_i = scan.getspec(i) # Fetch the `Spectrum` for integration `i`.\n",
    "    s_i.baseline(1, model=\"poly\", exclude=exclude, remove=True) # Fit a baseline model.\n",
    "    scan.calibrated[i] -= s_i.baseline_model(s_i.spectral_axis).value # Subtract the baseline model from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "Plot the middle integration of the middle scan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_bsub_int = sb0[len(sb0)//2].getspec(nint//2)\n",
    "sp_spec_bsub_int = spec_bsub_int.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "Now the continuum has been removed.\n",
    "\n",
    "We leave it as an exercise to repeat the above for every scan in the OTF map.\n",
    "The answer is hidden below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Answer</summary>\n",
    "    <pre>\n",
    "        <code class=\"Python\">\n",
    "            for i,_s in enumerate(sb0):\n",
    "                for j,_c in enumerate(_s._calibrated):\n",
    "                    if math.isnan(_c.data.sum()):\n",
    "                        # If the sum is NaN, then skip (continue) this item.\n",
    "                        continue\n",
    "                    s_i = getspec(j)\n",
    "                    s_i.baseline(1, model=\"poly\", exclude=exclude, remove=True)\n",
    "                    _s._calibrated[j] -= s_i.baseline_model(s_i.spectral_axis).value\n",
    "        </code>\n",
    "    </pre>\n",
    "    This can be speed up if we only create a <code>Spectrum</code> once, and then update its data attribute with the data for each integration before computing the baseline. That would be:\n",
    "    <pre>\n",
    "        <code class=\"Python\">\n",
    "            sp0 = sb0.timeaverage() \n",
    "            for i,_s in enumerate(sb0):\n",
    "                for j,_c in enumerate(_s._calibrated):\n",
    "                    if math.isnan(_c.data.sum()):\n",
    "                        # If the sum is NaN, then skip (continue) this item.\n",
    "                        continue\n",
    "                    sp0._data = _s._calibrated[j] # Update the data of the `Spectrum`.\n",
    "                    sp0.baseline(1, model=\"poly\", exclude=exclude, remove=True)\n",
    "                    _s._calibrated[j] -= sp0.baseline_model(sp0.spectral_axis).value\n",
    "        </code>\n",
    "    </pre>\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "## Writing the Data\n",
    "\n",
    "After calibration, we write the data to disk in SDFITS format so it can be gridded by the [`gbtgridder`](https://github.com/GreenBankObservatory/gbtgridder) (GBO's supported data gridding tool)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "sb0.write(\"otf_calibrated.fits\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "print_tsys",
     "test"
    ]
   },
   "source": [
    "## Gridding\n",
    "\n",
    "To grid GBT data GBO supports the use of the [`gbtgridder`](https://github.com/GreenBankObservatory/gbtgridder).\n",
    "This is not included as part of `dysh`.\n",
    "\n",
    "If you don't have it installed, here's a super short blurb how to get it, with the note that it is important to get the correct release branch. \n",
    "\n",
    "```bash\n",
    "git clone -b release_3.0  https://github.com/GreenBankObservatory/gbtgridder\n",
    "cd gbtgridder\n",
    "pip install .\n",
    "```\n",
    "\n",
    "This was the situation in the summer of 2025, and it may change, be sure to be in touch with the gbtgridder developers.\n",
    "\n",
    "After installation, either from the shell, or from the notebook, one can grid as follows:\n",
    "\n",
    "```bash\n",
    "gbtgridder --size 32 32  --channels 500:3500 -o otf --auto otf_calibrated.fits\n",
    "```\n",
    "This is telling the gridder to produce an output cube with 32 by 32 pixels using only channels between 500 and 3500, and to use \"otf\" as the name of the output files. The `--auto` part is to skip a confirmation prompt. The last argument, `otf_calibrated.fits`, is the input SDFITS (which was created in the previous cell).\n",
    "This call to the `gbtgridder` will create two files: `otf_cube.fits` and `otf_weight.fits`.\n",
    "The first file contains the gridded data, and the second the weights used during the gridding process.\n",
    "\n",
    "### Working with Cubes\n",
    "\n",
    "`dysh` is not meant to work with image cubes, there are plenty of great tools for this.\n",
    "Here we show how to use `astropy` to load the FITS cube and visualize its contents.\n",
    "We use `astropy.io.fits` to load the data, `astropy.wcs.WCS` to generate a World Coordinate System (WCS) representation out of the FITS header, this is handy for plotting.\n",
    "And, `matplotlib.pyplot` to plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.io import fits\n",
    "from astropy.wcs import WCS\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "Before proceeding we download the cube, in case you did not make your own.\n",
    "\n",
    "Note that this cube was made using data which had the baseline subtracted using a per integration model. \n",
    "If you generate your own cubes, there might be differences with respect to the cube used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "cube_filename = dysh_data(example='mapping-L/outputs/otf_cube.fits')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "Open the FITS cube, extract the data and header and then create a WCS object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "with fits.open(cube_filename) as hdu:\n",
    "    data = hdu[0].data\n",
    "    head = hdu[0].header\n",
    "wcs = WCS(head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "The data object is an array with 4 dimensions, the first being the Stokes axis, the second the spectral axis, the third the latitude (e.g., Dec), and the fourth the longitude (e.g., RA). Altough the FITS cube has a Stokes axis, the `gbtgridder` does not handle polarizations properly, so if you have multiple polarizations in the input SDFITS file(s), the `gbtgridder` will average them.\n",
    "\n",
    "To plot the spectrum from the center pixel we use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(data[0,:,16,16])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "Now plot an image with the mean of the spectral axis.\n",
    "We use the projection argument of `fig.add_subplot` to tell `matplotlib` to use the projection defined by the WCS object.\n",
    "This takes care of using sky coordinates in the figure.\n",
    "While plotting, `imshow`, we tell `matplotlib` to put the origin of the data, pixel (0,0), in the lower left corner (`origin=\"lower\"`), and to automatically adjust the aspect ratio (`aspect=\"auto\"`) for the axes (not really needed since the data is already a square)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(dpi=150)\n",
    "ax = fig.add_subplot(111, projection=wcs.celestial)\n",
    "ax.imshow(data[0].mean(axis=0), origin=\"lower\", aspect=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "Plot the data for channel 1500. \n",
    "We use the celestial representation of the WCS object (`wcs.celestial`) to ignore the non celestial axes (e.g., Stokes and spectral)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(dpi=150)\n",
    "ax = fig.add_subplot(111, projection=wcs.celestial)\n",
    "ax.imshow(data[0,1500], origin=\"lower\", aspect=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "Plot a PV diagram.\n",
    "Since we have multiple WCS axes (4), we use the `slices` argument, which tells the WCS object which axes to use for which axis. In this case the first WCS axis (longitude) goes in the y-axis of the figure, the latitude is fixed to its value at pixel 16, the third dimension (spectral axis) is shown in the x-axis, and the last dimensions (Stokes) is fixed to its value at pixel 0 (the only possibility in this case).\n",
    "\n",
    "\n",
    "See these links for more details on how to use `astropy` for plotting cubes with multiple WCS axes: [link1](https://docs.astropy.org/en/stable/visualization/wcsaxes/slicing_datacubes.html#slicing-the-wcs-object), and [link2](https://docs.astropy.org/en/stable/api/astropy.visualization.wcsaxes.WCSAxes.html#astropy.visualization.wcsaxes.WCSAxes).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(dpi=150)\n",
    "ax = fig.add_subplot(111, projection=wcs, slices=('y', 16, 'x', 0))\n",
    "ax.imshow(data[0,:,:,16].T, origin=\"lower\", aspect=\"auto\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "toc": {
   "base_numbering": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
